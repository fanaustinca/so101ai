{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3Cvi-mPwKou",
    "outputId": "2b8ec033-b8a2-4a88-b65e-55f5891d0bf4"
   },
   "outputs": [],
   "source": [
    "%conda install python-dotenv wandb -y\n",
    "\n",
    "# Setting lerobot env\n",
    "from lerobot_util import setup_lerobot_env\n",
    "\n",
    "setup_lerobot_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_repo=\"fanaustinca/fll_v3_batch_\"\n",
    "n_datasets = 2\n",
    "combined_repo_id = \"fanaustinca/fll_v3_combined\"\n",
    "\n",
    "# Create the list of repo IDs: ['repo_1', 'repo_2', ...]\n",
    "repo_list = [f\"{base_repo}{i}\" for i in range(1, n_datasets + 1)]\n",
    "    \n",
    "# Format the list as a string compatible with the CLI argument parser\n",
    "# Note: LeRobot often expects the list as a string representation of a Python list\n",
    "repo_str = str(repo_list).replace(\" \", \"\")\n",
    "    \n",
    "command_args = [\n",
    "    \"lerobot-edit-dataset\",\n",
    "    f\"--repo_id {combined_repo_id}\",\n",
    "    \"--operation.type merge\",\n",
    "    f\"--operation.repo_ids \\\"{repo_str}\\\"\",\n",
    "    \"--push_to_hub true\"\n",
    "]\n",
    "\n",
    "command = \" \".join(command_args)\n",
    "\n",
    "print(f\"Running command: {command}\")\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nWTgWpy60PN9",
    "outputId": "9f78f520-b99c-4d15-fab5-b842c4ba85c8"
   },
   "outputs": [],
   "source": [
    "from lerobot_util import cd_lerobot, print_shell_md\n",
    "import json\n",
    "import sys\n",
    "\n",
    "cd_lerobot()\n",
    "\n",
    "# Training ACT model.\n",
    "# Adjust the following variables before starting training\n",
    "tag = '100episodes'\n",
    "policy_type = 'act'\n",
    "\n",
    "if policy_type == 'diffusion':\n",
    "    n_action_steps = 10\n",
    "    chunk_size = 50\n",
    "elif policy_type == 'act':\n",
    "    n_action_steps = 50\n",
    "    chunk_size = 100\n",
    "else:\n",
    "    raise ValueError(f\"policy_type can only be 'act' or 'diffusion', got: {policy_type}\")\n",
    "\n",
    "hf_namespace = 'fanaustinca'\n",
    "training_repo = f\"{hf_namespace}/fll_v3_combined\"\n",
    "model_repo = f\"{hf_namespace}/fll_act_model_100episodes_v3\"\n",
    "# Use 'local_output' consistently\n",
    "local_output = f\"outputs/model/{model_repo}\"\n",
    "steps = 100000\n",
    "batch_size = 64\n",
    "num_workers = 12\n",
    "enable_image_transforms = False\n",
    "\n",
    "# Clean up output from last run\n",
    "!rm -rf {local_output}\n",
    "\n",
    "tfs_config = {\n",
    "  \"to_tensor\": {\"type\": \"ToImage\"},\n",
    "  \"affine\": {\n",
    "    \"type\": \"RandomAffine\",\n",
    "    \"kwargs\": {\"degrees\": 0, \"translate\": [0.05, 0.05]}\n",
    "  }\n",
    "}\n",
    "\n",
    "# Convert to a compact JSON string for the CLI\n",
    "tfs_string = json.dumps(tfs_config, separators=(',', ':'))\n",
    "\n",
    "arguments = [\n",
    "    \"src/lerobot/scripts/lerobot_train.py\",\n",
    "    f\"--policy.type={policy_type}\",\n",
    "    f\"--dataset.repo_id={training_repo}\",\n",
    "    f\"--job_name=fll_training_{policy_type}\",\n",
    "    f\"--output_dir={local_output}\",\n",
    "    f\"--policy.chunk_size={chunk_size}\",\n",
    "    f\"--policy.n_action_steps={n_action_steps}\",\n",
    "    \"--policy.device=cuda\",\n",
    "    f\"--steps={steps}\",\n",
    "    \"--save_freq=5000\",\n",
    "    f\"--batch_size={batch_size}\",\n",
    "    \"--log_freq=100\",\n",
    "    f\"--num_workers={num_workers}\",\n",
    "    \"--policy.use_amp=true\",\n",
    "    \"--policy.push_to_hub=true\",\n",
    "    f\"--policy.repo_id={model_repo}\",    \n",
    "    \"--wandb.enable=true\"\n",
    "]\n",
    "\n",
    "if enable_image_transforms:\n",
    "    arguments += [\n",
    "        \"--dataset.image_transforms.enable=true\",\n",
    "        \"--dataset.image_transforms.random_order=true\"\n",
    "    ]\n",
    "\n",
    "if policy_type == 'diffusion':\n",
    "    arguments += [\"--policy.num_inference_steps=30\"]\n",
    "\n",
    "# Your variables\n",
    "print_shell_md(\"Run the following command to start training:\",\n",
    "               f\"{sys.executable}\",\n",
    "               *arguments)\n",
    "\n",
    "print_shell_md(\n",
    "    \"After training is complete, run the following command to create a tag for the trained model:\",\n",
    "    f\"hf repo tag create {model_repo} {tag} -m \\\"Diffusion model trained on {training_repo}, tag {tag}\\\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

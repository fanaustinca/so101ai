{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Detect if we are in Colab or a generic cloud environment (like Vast.ai)\n",
    "is_colab = 'google.colab' in sys.modules \n",
    "is_vast = os.path.exists('/root/.vast_container') # Common Vast.ai indicator\n",
    "is_cloud = is_colab or is_vast\n",
    "\n",
    "if is_cloud:\n",
    "  py_url = \"https://raw.githubusercontent.com/fanaustinca/so101ai/refs/heads/main/lerobot_setup.py\"\n",
    "\n",
    "  !curl -O {py_url}\n",
    "  if os.path.exists(\"lerobot_setup.py\"):\n",
    "    print(\"\\nSuccessfully downloaded lerobot_setup.py\")\n",
    "  else:\n",
    "    print(\"\\nDownload failed.\")\n",
    "else:\n",
    "  print(\"Running on local machine. Skipping cloud setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3Cvi-mPwKou",
    "outputId": "2b8ec033-b8a2-4a88-b65e-55f5891d0bf4"
   },
   "outputs": [],
   "source": [
    "# Setting lerobot env\n",
    "from lerobot_setup import setup_lerobot_env\n",
    "\n",
    "setup_lerobot_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrIGMU4xQ8c3"
   },
   "outputs": [],
   "source": [
    "# Prepare training data set. Combine multiple batches under one training data.\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Generate input_datasets using a loop\n",
    "input_datasets = [f'fanaustinca/fll_diffusion_v2_batch_{i:02d}' for i in range(1, 3)]\n",
    "\n",
    "output_dataset_name = 'fanaustinca/fll_training_400episodes_test'\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "datasets_list = []\n",
    "for dataset_name in input_datasets:\n",
    "    print(f\"  Loading {dataset_name}\")\n",
    "    dataset = load_dataset(dataset_name, trust_remote_code=True)\n",
    "    # Assuming the dataset is a DatasetDict and we want to combine the 'train' split\n",
    "    # Adjust if your dataset structure is different (e.g., just a Dataset object)\n",
    "    if 'train' in dataset:\n",
    "        datasets_list.append(dataset['train'])\n",
    "    else:\n",
    "        datasets_list.append(dataset)\n",
    "\n",
    "# Concatenate datasets\n",
    "print(\"Concatenating datasets...\")\n",
    "combined_dataset = concatenate_datasets(datasets_list)\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "print(f\"Pushing combined dataset to {output_dataset_name}...\")\n",
    "combined_dataset.push_to_hub(output_dataset_name, private=False)\n",
    "print(\"Dataset combined and pushed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nWTgWpy60PN9",
    "outputId": "9f78f520-b99c-4d15-fab5-b842c4ba85c8"
   },
   "outputs": [],
   "source": [
    "from lerobot_setup import cd_lerobot\n",
    "import json\n",
    "import sys\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "cd_lerobot()\n",
    "\n",
    "# Training ACT model.\n",
    "# Adjust the following variables before starting training\n",
    "tag = '400episodes'\n",
    "policy_type = 'diffusion'\n",
    "n_action_steps = 10 if policy_type == 'diffusion' else 100\n",
    "hf_namespace = 'fanaustinca'\n",
    "training_repo = f\"{hf_namespace}/fll_training_combined\"\n",
    "model_repo = f\"{hf_namespace}/fll_diffusion_model_400episodes_v2\"\n",
    "# Use 'local_output' consistently\n",
    "local_output = f\"outputs/model/{model_repo}\"\n",
    "steps = 150000\n",
    "batch_size = 64\n",
    "num_workers = 16\n",
    "enable_image_transforms = True\n",
    "\n",
    "if policy_type not in ['act', 'diffusion']:\n",
    "    raise ValueError(f\"policy_type can only be 'act' or 'diffusion', got: {policy_type}\")\n",
    "\n",
    "\n",
    "# Clean up output from last run\n",
    "!rm -rf {local_output}\n",
    "\n",
    "tfs_config = {\n",
    "  \"to_tensor\": {\"type\": \"ToImage\"},\n",
    "  \"affine\": {\n",
    "    \"type\": \"RandomAffine\",\n",
    "    \"kwargs\": {\"degrees\": 0, \"translate\": [0.05, 0.05]}\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "    --dataset.image_transforms.enable=true \\\n",
    "    --dataset.image_transforms.random_order=false \\\n",
    "    --dataset.image_transforms.tfs='{tfs_string}' \\\n",
    "\"\"\"\n",
    "\n",
    "# Convert to a compact JSON string for the CLI\n",
    "tfs_string = json.dumps(tfs_config, separators=(',', ':'))\n",
    "\n",
    "arguments = f\"\"\"\\\n",
    "    --policy.type={policy_type} \\\n",
    "    --dataset.repo_id={training_repo} \\\n",
    "    --job_name=fll_training_{policy_type} \\\n",
    "    --output_dir={local_output} \\\n",
    "    --policy.n_action_steps={n_action_steps} \\\n",
    "    --policy.device=cuda \\\n",
    "    --steps={steps} \\\n",
    "    --save_freq=5000 \\\n",
    "    --batch_size={batch_size} \\\n",
    "    --log_freq=100 \\\n",
    "    --num_workers={num_workers} \\\n",
    "    --policy.use_amp=true \\\n",
    "    --policy.push_to_hub=true \\\n",
    "    --policy.repo_id={model_repo} \\\n",
    "    --dataset.image_transforms.enable=true \\\n",
    "    --dataset.image_transforms.random_order=true \\\n",
    "    --wandb.enable=true\"\"\"\n",
    "\n",
    "if enable_image_transforms:\n",
    "    arguments += \" --dataset.image_transforms.enable=true \\\n",
    "      --dataset.image_transforms.random_order=true\"\n",
    "\n",
    "if policy_type == 'diffusion':\n",
    "    arguments += \" --policy.num_inference_steps=30\"\n",
    "\n",
    "# Your variables\n",
    "command_str = f\"{sys.executable} src/lerobot/scripts/lerobot_train.py {arguments}\"\n",
    "\n",
    "# Wrap it in markdown code blocks\n",
    "display(Markdown(f\"```bash\\n{command_str}\\n```\"))\n",
    "\n",
    "command_str = f\"hf repo tag create {model_repo} {tag} -m \\\"Diffusion model trained on {training_repo}, tag {tag}\\\"\"\n",
    "display(Markdown(f\"```bash\\n{command_str}\\n```\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python lerobot",
   "language": "python",
   "name": "lerobot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
